# ğŸ¥ PredicciÃ³n de Asistencia a Citas MÃ©dicas â€“ Hospital MarÃ­a Auxiliadora

Este proyecto implementa un sistema de **Machine Learning supervisado** para predecir si un paciente asistirÃ¡ o no a su cita mÃ©dica en el Hospital de Apoyo MarÃ­a Auxiliadora (Lima, PerÃº).  
Se sigue la metodologÃ­a **CRISP-DM** e incluye las fases **1 a 5**, desarrolladas Ã­ntegramente en Python.

ğŸ”— **Fuente de datos:**  
Portal de Datos Abiertos del Gobierno del PerÃº  
https://www.datosabiertos.gob.pe/dataset/citas-medicas-en-el-hospital-de-apoyo-maria-auxiliadora-hma

---

## ğŸ¯ Objetivo del Proyecto

Desarrollar un modelo que prediga la asistencia de pacientes a sus citas mÃ©dicas utilizando informaciÃ³n:

- Administrativa  
- DemogrÃ¡fica  
- GeogrÃ¡fica  
- Temporal  
- De comportamiento (patrones histÃ³ricos)

Esta predicciÃ³n sirve para:

- Reducir inasistencias y tiempos muertos.  
- Optimizar disponibilidad de mÃ©dicos e infraestructura.  
- Mejorar la eficiencia del flujo de atenciÃ³n.  
- Soportar decisiones hospitalarias en planificaciÃ³n diaria y semanal.

---

## âš™ï¸ MetodologÃ­a CRISP-DM

| Fase | DescripciÃ³n |
|------|-------------|
| **1. ComprensiÃ³n del negocio** | AnÃ¡lisis del problema de inasistencias, impacto operativo y objetivos predictivos. |
| **2. ComprensiÃ³n de los datos** | AnÃ¡lisis exploratorio, validaciÃ³n de la calidad de datos, distribuciÃ³n de variables y correlaciones. |
| **3. PreparaciÃ³n de datos** | Limpieza, codificaciÃ³n, normalizaciÃ³n, ingenierÃ­a de caracterÃ­sticas y manejo de desbalance. |
| **4. Modelado** | Entrenamiento de modelos LightGBM y XGBoost con mÃºltiples semillas y validaciÃ³n cruzada. |
| **5. EvaluaciÃ³n** | ComparaciÃ³n de modelos individuales y ensamblajes, anÃ¡lisis detallado de mÃ©tricas. |
| **6. Despliegue (propuesto)** | IntegraciÃ³n futura con sistemas hospitalarios para apoyo en la toma de decisiones. |

---

## ğŸ§ª Modelado Implementado

El proyecto emplea dos modelos de **Gradient Boosting** altamente eficientes para datos tabulares:

### ğŸ”¹ LightGBM
- Training acelerado  
- Eficiente con datasets grandes  
- Buen rendimiento con variables categÃ³ricas codificadas  

### ğŸ”¹ XGBoost
- Robusto en presencia de ruido  
- RegularizaciÃ³n L1/L2  
- Convergencia rÃ¡pida  
- Entrenado con aceleraciÃ³n GPU (CUDA)  

---

## ğŸ² Estrategia Especial de Entrenamiento

### âœ” Uso de MÃºltiples Semillas (42, 123 y 456)
Para garantizar estabilidad y reducir varianza:

- Cada semilla genera particiones independientes  
- Se entrena un modelo LGBM y un XGB por semilla  
- **Total: 6 modelos individuales**

### âœ” ParticiÃ³n de Datos en Dos Niveles
- **Test (no visto): 15%**  
- **Train + Validation: 85%**  
  - Dentro: **85% train**, **15% validaciÃ³n** por semilla

### âœ” Balanceo con SMOTETomek
- Corrige desbalance  
- Mejora el recall de la clase minoritaria  

### âœ” ValidaciÃ³n cruzada estratificada (5-fold)
- Aplicada por cada modelo (semilla Ã— algoritmo)  
- Utiliza el nÃºmero Ã³ptimo de Ã¡rboles obtenido por **early stopping**  

---

## ğŸ§¬ IngenierÃ­a de CaracterÃ­sticas

Variables derivadas creadas:

- `Diferencia_dias` (solicitud â†’ cita)
- `mes_cita`
- `semana_mes_cita`
- `bimestre_cita`
- `trimestre_cita`
- `semestre_cita`
- `estacion_cita` (verano/otoÃ±o/invierno/primavera)
- `Cita_mes_diferente`

Variables finales incluidas en el modelo:

- Datos demogrÃ¡ficos  
- Datos administrativos  
- Datos temporales  
- Variables codificadas y estandarizadas  

---

## ğŸ¤– Ensamblajes de Modelos

Se implementaron dos estrategias:

### ğŸ”¸ 1. Ensamblaje Ponderado por Semilla (50% LGBM + 50% XGB)
- 3 ensamblajes individuales (uno por semilla)
- 1 ensamblaje final agregado

### ğŸ”¸ 2. Ensamblaje por VotaciÃ³n Mayoritaria
- **LGBM Ensemble** (promedio de sus 3 semillas)
- **XGB Ensemble** (promedio de sus 3 semillas)
- **Global Ensemble** (50% LGBM Ensemble + 50% XGB Ensemble)

---

## ğŸ† Modelo Final Seleccionado

### â­ **Global Ensemble (VotaciÃ³n Mayoritaria)**

Seleccionado por:

- **F1 Score mÃ¡s alto:** 0.8173  
- **Accuracy mÃ¡s alto:** 0.8193  
- **ROC AUC:** 0.8864  
- **Recall en clase positiva:** 92.32%  
- Menor varianza entre semillas  
- Mejor robustez general  

Este mÃ©todo combina la fortaleza de:

- **LightGBM â†’** mayor precisiÃ³n en clase negativa  
- **XGBoost â†’** mayor recall y mejor discriminaciÃ³n  

---

## ğŸ“Š Resultados Finales

### ğŸ” MÃ©tricas del mejor ensamblaje (Global Ensemble)

| MÃ©trica | Resultado |
|---------|-----------|
| **Accuracy** | 0.8193 |
| **F1 Weighted** | 0.8173 |
| **ROC AUC** | 0.8864 |
| **Recall clase 1** | 0.9232 |
| **Precision clase 1** | 0.7643 |

---

## ğŸ“ Requisitos del Proyecto

- Python 3.10+  
- Pandas  
- NumPy  
- Scikit-learn  
- LightGBM  
- XGBoost (GPU opcional)  
- Imbalanced-learn  
- Matplotlib / Seaborn  

---

## ğŸ“š CrÃ©ditos AcadÃ©micos

Proyecto desarrollado con fines educativos como parte de una investigaciÃ³n en Ciencia de Datos, usando datos abiertos del **Ministerio de Salud del PerÃº (MINSA)**.

---







